{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Price prediction[TOP12%]_ Simple Regularization.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JPLAVALLEY/kaggle/blob/master/Price_prediction%5BTOP12_%5D__Simple_Regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS7jZKSXZm3P",
        "colab_type": "text"
      },
      "source": [
        "# COMPONENTS OF THIS PROJECT\n",
        "\n",
        "### 1- EXPLORATORY DATA ANALYSIS\n",
        "\n",
        "### 2- DATA CLEANING\n",
        "\n",
        "### 3- FEATURE ENGINEERING\n",
        "\n",
        "### 4- ENCODING CATEGORICAL FEATURES\n",
        "\n",
        "### 5- DETECTING OUTLIERS\n",
        "\n",
        "### 6- MACHINE LEARNING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "_kg_hide-input": true,
        "id": "BDN9Ps7PZm3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore')\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW8qSRp5Zm3Z",
        "colab_type": "text"
      },
      "source": [
        "We open our train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "PfEr99BAZm3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\n",
        "b = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "HRPRDPr7Zm3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Use this code to show all the 163 columns\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWiJhOMzZm3g",
        "colab_type": "text"
      },
      "source": [
        "Here is a glimpse of what we will be dealing with:\n",
        "* Many features, many missing values and one target feature: \"SalePrice\" which is the price of the houses we are supposed to predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "QscZQwQsZm3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "axr-Bsq2Zm3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('The shape of our training set: ',a.shape[0], 'houses', 'and', a.shape[1], 'features')\n",
        "print('The shape of our testing set: ',b.shape[0], 'houses', 'and', b.shape[1], 'features')\n",
        "print('The testing set has 1 feature less than the training set, which is SalePrice, the target to predict  ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfAqWFauZm3n",
        "colab_type": "text"
      },
      "source": [
        "# Exploratory data analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA_RM8G3Zm3o",
        "colab_type": "text"
      },
      "source": [
        "Let's have a look first at the correlation between numerical features and the target \"SalePrice\", in order to have a first idea of the connections between features. Just by looking at the heatmap below we can see many light colors, many features have high correlation with the target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "MheWs-RMZm3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num=a.select_dtypes(exclude='object')\n",
        "numcorr=num.corr()\n",
        "f,ax=plt.subplots(figsize=(19,1))\n",
        "sns.heatmap(numcorr.sort_values(by=['SalePrice'], ascending=False).head(1),annot=True, fmt = \".2f\")\n",
        "plt.title(\" Numerical features correlation with the sale price\", weight='bold', fontsize=18)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e7taAO_Zm3s",
        "colab_type": "text"
      },
      "source": [
        "To have a better idea, we sort the features according to their correlation with the sale price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "-xVdLxfeZm3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Num=numcorr['SalePrice'].sort_values(ascending=False).head(10).to_frame()\n",
        "\n",
        "cm = sns.light_palette(\"cyan\", as_cmap=True)\n",
        "\n",
        "s = Num.style.background_gradient(cmap=cm)\n",
        "s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM-_sDlNZm3w",
        "colab_type": "text"
      },
      "source": [
        "Interesting! The overall quality, the living area, basement area, garage cars and garage area have the highest correlation values with the sale price, which is logical, better quality and bigger area = Higher price.\n",
        "* Also some features such as, full bath or 1st floor surface have a higher correlation, those are luxury features, more luxury = Higher price.\n",
        "* and Year built, the newer buildings seem to have higher sale prices.\n",
        "\n",
        "Let's dig in more into the data, those are just the numerical features. I assume that categorical features will be very important, for example, the neighborhood feature will be important, maybe the most important, given that good locations nowadays cost good money.\n",
        "\n",
        "* But before going any futher, we start by cleaning the data from missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "tWIQab88Zm3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.style.use('seaborn')\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "plt.subplots(0,0,figsize=(15,3))\n",
        "\n",
        "\n",
        "a.isnull().mean().sort_values(ascending=False).plot.bar(color='black')\n",
        "plt.axhline(y=0.1, color='r', linestyle='-')\n",
        "plt.title('Missing values average per column: Train set', fontsize=20, weight='bold' )\n",
        "plt.show()\n",
        "\n",
        "plt.subplots(1,0,figsize=(15,3))\n",
        "b.isnull().mean().sort_values(ascending=False).plot.bar(color='black')\n",
        "plt.axhline(y=0.1, color='r', linestyle='-')\n",
        "plt.title('Missing values average per column: Test set ', fontsize=20, weight='bold' )\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lmnRRIIZm30",
        "colab_type": "text"
      },
      "source": [
        "* Good news! Most of the features are clean from missing values\n",
        "\n",
        "* We combine first the train and test datasets to run all the data munging and feature engineering on both of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1OXiEaHIZm31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "na = a.shape[0]\n",
        "nb = b.shape[0]\n",
        "y_train = a['SalePrice'].to_frame()\n",
        "#Combine train and test sets\n",
        "c1 = pd.concat((a, b), sort=False).reset_index(drop=True)\n",
        "#Drop the target \"SalePrice\" and Id columns\n",
        "c1.drop(['SalePrice'], axis=1, inplace=True)\n",
        "c1.drop(['Id'], axis=1, inplace=True)\n",
        "print(\"Total size is :\",c1.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2RYFvpbZm34",
        "colab_type": "text"
      },
      "source": [
        "# Data cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GTQVCV0Zm35",
        "colab_type": "text"
      },
      "source": [
        "* First thing to do is get rid of the features with more than 90% missing values. For example the PoolQC's missing values are probably due to the lack of pools in some buildings, which is very logical. But replacing those (more than 90%) missing values with \"no pool\" will leave us with a feature with low variance, and low variance features are uniformative for machine learning models. So we drop the features with more than 90% missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "X-5EbeEjZm36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c=c1.dropna(thresh=len(c1)*0.9, axis=1)\n",
        "print('We dropped ',c1.shape[1]-c.shape[1], ' features in the combined set')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yESgXtOZm39",
        "colab_type": "text"
      },
      "source": [
        "Before cleaning the data, we zoom at the features with missing values, those missing values won't be treated iqually. Some features have barely 1 or 2 missing values, we will use the forward fill method to fill them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "MuuD0bOqZm3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "allna = (c.isnull().sum() / len(c))\n",
        "allna = allna.drop(allna[allna == 0].index).sort_values(ascending=False)\n",
        "plt.figure(figsize=(12, 8))\n",
        "allna.plot.barh(color='purple')\n",
        "plt.title('Missing values average per column', fontsize=25, weight='bold' )\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "KSouXX-GZm4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('The shape of the combined dataset after dropping features with more than 90% M.V.', c.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKqj0j6uZm4E",
        "colab_type": "text"
      },
      "source": [
        "We isolate the missing values from the rest of the dataset to have a good idea of how to treat them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ZP_Y0YsRZm4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NA=c[['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond','GarageYrBlt','BsmtFinType2','BsmtFinType1','BsmtCond', 'BsmtQual','BsmtExposure', 'MasVnrArea','MasVnrType','Electrical','MSZoning','BsmtFullBath','BsmtHalfBath','Utilities','Functional','Exterior1st','BsmtUnfSF','Exterior2nd','TotalBsmtSF','GarageArea','GarageCars','KitchenQual','BsmtFinSF2','BsmtFinSF1','SaleType']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p3iHQTJZm4I",
        "colab_type": "text"
      },
      "source": [
        "We split them to:\n",
        "* Categorical features\n",
        "* Numerical features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ZCisGaYGZm4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NAcat=NA.select_dtypes(include='object')\n",
        "NAnum=NA.select_dtypes(exclude='object')\n",
        "print('We have :',NAcat.shape[1],'categorical features with missing values')\n",
        "print('We have :',NAnum.shape[1],'numerical features with missing values')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3TJxIpVZm4M",
        "colab_type": "text"
      },
      "source": [
        "So, 18 categorical features and 10 numerical features to clean.\n",
        "* We start with the numerical features, first thing to do is have a look at them to learn more about their distribution and decide how to clean them:\n",
        "- Most of the features are going to be filled with 0s because we assume that they don't exist, for example GarageArea, GarageCars with missing values are simply because the house lacks a garage.\n",
        "- GarageYrBlt: Year garage was built can't be filled with 0s, so we fill with the median (1980)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "IAMLw2YkZm4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NAnum.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "iFh0iRmEZm4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#MasVnrArea: Masonry veneer area in square feet, the missing data means no veneer so we fill with 0\n",
        "c['MasVnrArea']=c.MasVnrArea.fillna(0)\n",
        "#GarageYrBlt:  Year garage was built, we fill the gaps with the median: 1980\n",
        "c['GarageYrBlt']=c[\"GarageYrBlt\"].fillna(1980)\n",
        "#For the rest of the columns: Bathroom, half bathroom, basement related columns and garage related columns:\n",
        "#We will fill with 0s because they just mean that the hosue doesn't have a basement, bathrooms or a garage"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fojE-2sUZm4U",
        "colab_type": "text"
      },
      "source": [
        "And we have 18 Categorical features with missing values:\n",
        "* Some features have just 1 or 2 missing values, so we will just use the forward fill method because they are obviously values that can't be filled with 'None's\n",
        "* Features with many missing values are mostly basement and garage related (same as in numerical features) so as we did with numerical features (filling them with 0s), we will fill the categorical missing values with \"None\"s assuming that the houses lack basements and garages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "THjCkH1BZm4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NAcat.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "eW4stuc6Zm4b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NAcat1= NAcat.isnull().sum().to_frame().sort_values(by=[0]).T\n",
        "cm = sns.light_palette(\"lime\", as_cmap=True)\n",
        "\n",
        "NAcat1 = NAcat1.style.background_gradient(cmap=cm)\n",
        "NAcat1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QwthrUFJZm4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We start with features having just few missing value:  We fill the gap with forward fill method:\n",
        "c['Electrical']=c['Electrical'].fillna(method='ffill')\n",
        "c['SaleType']=c['SaleType'].fillna(method='ffill')\n",
        "c['KitchenQual']=c['KitchenQual'].fillna(method='ffill')\n",
        "c['Exterior1st']=c['Exterior1st'].fillna(method='ffill')\n",
        "c['Exterior2nd']=c['Exterior2nd'].fillna(method='ffill')\n",
        "c['Functional']=c['Functional'].fillna(method='ffill')\n",
        "c['Utilities']=c['Utilities'].fillna(method='ffill')\n",
        "c['MSZoning']=c['MSZoning'].fillna(method='ffill')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScoLpTQwZm4g",
        "colab_type": "text"
      },
      "source": [
        "* We dealt already with small missing values or values that can't be filled with \"0\" such as Garage year built.\n",
        "* The rest of the features are mostly basement and garage related with 100s of missing values, we will just fill 0s in the numerical features and 'None' in categorical features, assuming that the houses don't have basements, full bathrooms or garage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8bfTm9tjZm4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Categorical missing values\n",
        "NAcols=c.columns\n",
        "for col in NAcols:\n",
        "    if c[col].dtype == \"object\":\n",
        "        c[col] = c[col].fillna(\"None\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QCqE2DxBZm4l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Numerical missing values\n",
        "for col in NAcols:\n",
        "    if c[col].dtype != \"object\":\n",
        "        c[col]= c[col].fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kfK4DN3tZm4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c.isnull().sum().sort_values(ascending=False).head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_yupFtPZm4r",
        "colab_type": "text"
      },
      "source": [
        "We finally end up with a clean dataset, next thing to do is create new features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66xOB-AmZm4s",
        "colab_type": "text"
      },
      "source": [
        "# Feature engineering:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQRG26EwZm4s",
        "colab_type": "text"
      },
      "source": [
        "Since the area is a very important variable, we will create a new feature \"TotalArea\" that sums the area of all the floors and the basement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-u1D1lbYZm4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c['TotalArea'] = c['TotalBsmtSF'] + c['1stFlrSF'] + c['2ndFlrSF'] + c['GrLivArea']\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyJcvv_6Zm4v",
        "colab_type": "text"
      },
      "source": [
        "Feature engineering is very important to improve the model's performance, I will start in this kernel just with the TotalArea feature and will keep updating the kernel by creating new features.\n",
        "* ** This part of the kernel is not finished yet.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q51IbC-HZm4w",
        "colab_type": "text"
      },
      "source": [
        "# Encoding categorical features\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mEFPp-aRZm4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cb=pd.get_dummies(c)\n",
        "print(\"the shape of the original dataset\",c.shape)\n",
        "print(\"the shape of the encoded dataset\",cb.shape)\n",
        "print(\"We have \",cb.shape[1]- c.shape[1], 'new encoded features')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "CBAmnlaOZm4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#log transform skewed numeric features:\n",
        "from scipy.stats import skew\n",
        "\n",
        "numeric_feats = c.dtypes[c.dtypes != \"object\"].index\n",
        "\n",
        "skewed_feats = c[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
        "skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
        "skewed_feats = skewed_feats.index\n",
        "\n",
        "c[skewed_feats] = np.log1p(c[skewed_feats])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_coQc7kZm42",
        "colab_type": "text"
      },
      "source": [
        "We are done with the cleaning and feature engineering. Now, we split the combined dataset to the original train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "W8dLcUVrZm43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Train = cb[:na]  #na is the number of rows of the original training set\n",
        "Test = cb[na:] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTtajvt7Zm45",
        "colab_type": "text"
      },
      "source": [
        "# Detecting outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-HC5cssZm45",
        "colab_type": "text"
      },
      "source": [
        "This part of the kernel will be a little bit messy. I didn't want to deal with the outliers in the combined dataset to keep the shape of the original train and test datasets. Dropping them would shift the location of the rows.\n",
        "* If you know a better solution to this, I will be more than happy to read your recommandations.\n",
        "\n",
        "* OK. So we go back to our original train dataset to visualize the important features / Sale price scatter plot to find outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "R10cuNb2Zm46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(15,10))\n",
        "ax1 = plt.subplot2grid((2,2),(0,0))\n",
        "plt.scatter(x=a['GrLivArea'], y=a['SalePrice'], color=('yellowgreen'))\n",
        "plt.axvline(x=4600, color='r', linestyle='-')\n",
        "plt.title('Ground living Area- Price scatter plot', fontsize=15, weight='bold' )\n",
        "\n",
        "ax1 = plt.subplot2grid((2,2),(0,1))\n",
        "plt.scatter(x=a['TotalBsmtSF'], y=a['SalePrice'], color=('red'))\n",
        "plt.axvline(x=5900, color='r', linestyle='-')\n",
        "plt.title('Basement Area - Price scatter plot', fontsize=15, weight='bold' )\n",
        "\n",
        "ax1 = plt.subplot2grid((2,2),(1,0))\n",
        "plt.scatter(x=a['1stFlrSF'], y=a['SalePrice'], color=('deepskyblue'))\n",
        "plt.axvline(x=4000, color='r', linestyle='-')\n",
        "plt.title('First floor Area - Price scatter plot', fontsize=15, weight='bold' )\n",
        "\n",
        "ax1 = plt.subplot2grid((2,2),(1,1))\n",
        "plt.scatter(x=a['MasVnrArea'], y=a['SalePrice'], color=('gold'))\n",
        "plt.axvline(x=1500, color='r', linestyle='-')\n",
        "plt.title('Masonry veneer Area - Price scatter plot', fontsize=15, weight='bold' )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDmj_NIDZm49",
        "colab_type": "text"
      },
      "source": [
        "The outliers are the points in the right that have a larger area but a very low sale price. We localize those points by sorting their respective columns\n",
        "\n",
        "* Interesting! The outlier in \"basement\" and \"first floor\" features is the same as the first outlier in ground living area: **The outlier with index number 1298. **\n",
        "* We detect the outlier 297 in MasVnrArea."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "x_Zzdq6aZm4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a['GrLivArea'].sort_values(ascending=False).head(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "LzeKQqpMZm5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a['TotalBsmtSF'].sort_values(ascending=False).head(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "aFlVGmS9Zm5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a['MasVnrArea'].sort_values(ascending=False).head(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "G_rLB6BuZm5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a['1stFlrSF'].sort_values(ascending=False).head(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-hqkrWkZm5I",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We can safely remove those 3 points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9ATiWe8YZm5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=Train[(Train['GrLivArea'] < 4600) & (Train['MasVnrArea'] < 1500)]\n",
        "\n",
        "print('We removed ',Train.shape[0]- train.shape[0],'outliers')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWIXUV8jZm5K",
        "colab_type": "text"
      },
      "source": [
        "We do the same thing with \"SalePrice\" column, we localize those  rows 1298 and 523 and make sure they are the right outliers to remove. \n",
        "* They both have the same price range as the detected outliers. So, we just drop them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "tqqloXUiZm5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target=a[['SalePrice']]\n",
        "target.loc[1298]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Fa24uouGZm5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target.loc[523]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "jRM7oexYZm5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos = [1298,523, 297]\n",
        "target.drop(target.index[pos], inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8oPWkYmiZm5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('We make sure that both train and target sets have the same row number after removing the outliers:')\n",
        "print( 'Train: ',train.shape[0], 'rows')\n",
        "print('Target:', target.shape[0],'rows')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "tdkvbVBVZm5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.style.use('seaborn')\n",
        "sns.set_style('whitegrid')\n",
        "fig = plt.figure(figsize=(15,5))\n",
        "#1 rows 2 cols\n",
        "#first row, first col\n",
        "ax1 = plt.subplot2grid((1,2),(0,0))\n",
        "plt.scatter(x=a['GrLivArea'], y=a['SalePrice'], color=('yellowgreen'))\n",
        "plt.title('Area-Price plot with outliers',weight='bold', fontsize=18)\n",
        "plt.axvline(x=4600, color='r', linestyle='-')\n",
        "#first row sec col\n",
        "ax1 = plt.subplot2grid((1,2),(0,1))\n",
        "plt.scatter(x=train['GrLivArea'], y=target['SalePrice'], color='navy')\n",
        "plt.axvline(x=4600, color='r', linestyle='-')\n",
        "plt.title('Area-Price plot without outliers',weight='bold', fontsize=18)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-MKJzDEZm5Y",
        "colab_type": "text"
      },
      "source": [
        "#### Log tranform the target because it's skewed to the right"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "l0TeiS_SZm5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#log transform the target:\n",
        "target[\"SalePrice\"] = np.log1p(target[\"SalePrice\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "7D_re7GcZm5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.style.use('seaborn')\n",
        "sns.set_style('whitegrid')\n",
        "fig = plt.figure(figsize=(15,5))\n",
        "#1 rows 2 cols\n",
        "#first row, first col\n",
        "ax1 = plt.subplot2grid((1,2),(0,0))\n",
        "plt.hist(a.SalePrice, bins=10, color='mediumpurple')\n",
        "plt.title('Sale price distribution before normalization',weight='bold', fontsize=18)\n",
        "#first row sec col\n",
        "ax1 = plt.subplot2grid((1,2),(0,1))\n",
        "plt.hist(target.SalePrice, bins=10, color='darkcyan')\n",
        "plt.title('Sale price distribution after normalization',weight='bold', fontsize=18)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqAaofC_Zm5b",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RHwRpsd9Zm5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n",
        "import math\n",
        "import sklearn.model_selection as ms\n",
        "import sklearn.metrics as sklm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnVhItdbZm5g",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7hDeqWgZm5h",
        "colab_type": "text"
      },
      "source": [
        "We start machine learning by setting the features and target:\n",
        "* Features: x\n",
        "* Target: y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xZtqdIVfZm5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=train\n",
        "y=np.array(target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN2h748gZm5k",
        "colab_type": "text"
      },
      "source": [
        "Then, we split them to train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QOAwb1ABZm5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y,test_size = .33, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulM3KtHsZm5m",
        "colab_type": "text"
      },
      "source": [
        "We use RobustScaler to scale our data because it's powerful against outliers, we already detected some but there must be some other outliers out there, I will try to find them in future versions of the kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "sQGZnKgPZm5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "scaler= RobustScaler()\n",
        "# transform \"x_train\"\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "# transform \"x_test\"\n",
        "x_test = scaler.transform(x_test)\n",
        "#Transform the test set\n",
        "X_test= scaler.transform(Test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYM7xpJGZm5p",
        "colab_type": "text"
      },
      "source": [
        "We first start by trying the very basic regression model: Linear regression. \n",
        "* We use 5- Fold cross validation for a better error estimate:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rxLsxvvuZm5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lreg=LinearRegression()\n",
        "MSEs=ms.cross_val_score(lreg, x, y, scoring='neg_mean_squared_error', cv=5)\n",
        "meanMSE=np.mean(MSEs)\n",
        "print(meanMSE)\n",
        "print('RMSE = '+str(math.sqrt(-meanMSE)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxXgH8y1Zm5r",
        "colab_type": "text"
      },
      "source": [
        "We get an **RMSE=0.125.**\n",
        "* Our goal is to minimize the error, we use regularization methods: Ridge, Lasso and ElasticNet, in order to lower the squared error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FcEI0lCZm5r",
        "colab_type": "text"
      },
      "source": [
        "## Regularization: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lsdy5JzCZm5s",
        "colab_type": "text"
      },
      "source": [
        "> ## Ridge regression:\n",
        "* Minimize squared error + a term **alpha** that penalizes the error\n",
        "* We need to find a value of **alpha** that minimizes the train and test error (avoid overfitting)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1ucPMlPrZm5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn.model_selection as GridSearchCV\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "ridge=Ridge()\n",
        "parameters= {'alpha':[x for x in range(1,101)]}\n",
        "\n",
        "ridge_reg=ms.GridSearchCV(ridge, param_grid=parameters, scoring='neg_mean_squared_error', cv=5)\n",
        "ridge_reg.fit(x,y)\n",
        "print(\"The best value of Alpha is: \",ridge_reg.best_params_)\n",
        "print(\"The best score achieved with Alpha=14 is: \",ridge_reg.best_score_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "94a74_pwZm5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ridge_mod=Ridge(alpha=14)\n",
        "ridge_mod.fit(x_train,y_train)\n",
        "y_pred_train=ridge_mod.predict(x_train)\n",
        "y_pred_test=ridge_mod.predict(x_test)\n",
        "\n",
        "print('Root Mean Square Error train = ' + str(math.sqrt(sklm.mean_squared_error(y_train, y_pred_train))))\n",
        "print('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, y_pred_test))))   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNoSICtQZm5w",
        "colab_type": "text"
      },
      "source": [
        "Good! Ridge regression minimized the error. Ridge's **RMSE= 0.115**\n",
        "* Next we try Lasso regularization: Similar procedure as ridge regularization but Lasso tends to have a lot of 0 entries in it and just few nonzeros (easy selection). In other words, lasso drops the uninformative features and keeps just the important ones.\n",
        "* As with Ridge regularization, we need to find the **alpha** parameter that penalizes the error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99k-9U7fZm5w",
        "colab_type": "text"
      },
      "source": [
        "> ## Lasso regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_rrCDcKYZm5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "parameters= {'alpha':[0.0001,0.001,0.01,0.1,1,10,100]}\n",
        "\n",
        "lasso=Lasso()\n",
        "lasso_reg=ms.GridSearchCV(lasso, param_grid=parameters, scoring='neg_mean_squared_error', cv=5)\n",
        "lasso_reg.fit(x,y)\n",
        "\n",
        "print('The best value of Alpha is: ',lasso_reg.best_params_)\n",
        "print('The best score achieved with Alpha=0.001 is: ',math.sqrt(-lasso_reg.best_score_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eveXMNRZm5y",
        "colab_type": "text"
      },
      "source": [
        "I tried numbers that round our best Alpha=0.001 and found out that 0.009 gives a better result, so we continue with **Alpha=0.009**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "OOBs4uD4Zm5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lasso_mod=Lasso(alpha=0.0009)\n",
        "lasso_mod.fit(x_train,y_train)\n",
        "y_lasso_train=lasso_mod.predict(x_train)\n",
        "y_lasso_test=lasso_mod.predict(x_test)\n",
        "\n",
        "print('Root Mean Square Error train = ' + str(math.sqrt(sklm.mean_squared_error(y_train, y_lasso_train))))\n",
        "print('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, y_lasso_test))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bARkXFMjZm51",
        "colab_type": "text"
      },
      "source": [
        "With Lasso regularization, we achieved a better score. We minimized the error: **RMSE= 0.1120**\n",
        "* We check next, the important features that our model used to make predictions\n",
        "* The number of uninformative features that were dropped"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "pl9OHwhWZm52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "coefs = pd.Series(lasso_mod.coef_, index = x.columns)\n",
        "\n",
        "imp_coefs = pd.concat([coefs.sort_values().head(10),\n",
        "                     coefs.sort_values().tail(10)])\n",
        "imp_coefs.plot(kind = \"barh\", color='yellowgreen')\n",
        "plt.title(\"Features importance in the Lasso Model\")\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMgQDpC7Zm57",
        "colab_type": "text"
      },
      "source": [
        "Nice! The most important feature is the new feature we created \"**TotalArea**\". \n",
        "* Other features such as neighborhood or overall quality are among the main important features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UkXF7MB3Zm57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(\"Lasso kept \",sum(coefs != 0), \"important features and dropped the other \", sum(coefs == 0),\" features\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR4OCfQCZm59",
        "colab_type": "text"
      },
      "source": [
        "Next, we try ElasticNet. A regressor that combines both ridge and Lasso.\n",
        "We use cross validation to find:\n",
        "* Alpha\n",
        "* Ratio between Ridge and Lasso, for a better combination of both"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnHqW-OeZm5-",
        "colab_type": "text"
      },
      "source": [
        "> ## ElasticNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "B3Z9wIO0Zm5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import ElasticNetCV\n",
        "\n",
        "alphas = [0.001, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 1,10]\n",
        "l1ratio = [0.1, 0.3,0.5, 0.9, 0.95, 0.99, 1]\n",
        "\n",
        "elastic_cv = ElasticNetCV(cv=5, max_iter=1e7, alphas=alphas,  l1_ratio=l1ratio)\n",
        "\n",
        "elasticmod = elastic_cv.fit(x_train, y_train.ravel())\n",
        "ela_pred=elasticmod.predict(x_test)\n",
        "print('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, ela_pred))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHI4yUjZZm6E",
        "colab_type": "text"
      },
      "source": [
        "ElasticNet gives a higher error score than Lasso with **RMSE=0.114**. But we will definitely use it later in stacking and averaging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFI7Y6bLZm6F",
        "colab_type": "text"
      },
      "source": [
        "We will try other kind of regressors, such as XGBRegressor and ExtraTreesRegressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-HiT83B7Zm6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from xgboost.sklearn import XGBRegressor\n",
        "\n",
        "#xg_reg = XGBRegressor()\n",
        "#xgparam_grid= {'learning_rate' : [0.01],'n_estimators':[3460],\n",
        "#                                     'max_depth':[3], 'min_child_weight':[3,5],\n",
        "#                                     'colsample_bytree':[0.5,0.7],\n",
        "#                                     'reg_alpha':[0.0001,0.001,0.01,0.1,10,100],\n",
        "#                                    'reg_lambda':[1,0.01,0.001,0.0001]}\n",
        "#\n",
        "#xg_grid=GridSearchCV(xg_reg, param_grid=xgparam_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "#xg_grid.fit(x_train,y_train)\n",
        "#print(xg_grid.best_estimator_)\n",
        "#print(xg_grid.best_score_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPIG_4CgZm6L",
        "colab_type": "text"
      },
      "source": [
        "The gridSearch above tunes the hyperparamaters, but it takes forever to run. I copy the best estimator results to the model below. Feel free to uncomment and check it out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JBWZWFg4Zm6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xgb= XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "             colsample_bynode=1, colsample_bytree=0.5, gamma=0,\n",
        "             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n",
        "             max_depth=3, min_child_weight=3, missing=None, n_estimators=3460,\n",
        "             n_jobs=1, nthread=None, objective='reg:squarederror', random_state=0,\n",
        "             reg_alpha=0.0001, reg_lambda=0.01, scale_pos_weight=1, seed=None,\n",
        "             silent=None, subsample=1, verbosity=1)\n",
        "xgmod=xgb.fit(x_train,y_train)\n",
        "xg_pred=xgmod.predict(x_test)\n",
        "print('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, xg_pred))))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdtnPA9xZm6V",
        "colab_type": "text"
      },
      "source": [
        "* XGB score **RMSE= 0.117**\n",
        "* Worst than Ridge and Lasso but we will use is in stacking regressors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "HM7ItOCiZm6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "ex_reg=ExtraTreesRegressor(n_estimators=2000, max_depth=20)\n",
        "\n",
        "ex_mod=ex_reg.fit(x_train,y_train.ravel())\n",
        "ex_pred=ex_mod.predict(x_test)\n",
        "\n",
        "print('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, ex_pred))))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oorNqw6FZm6X",
        "colab_type": "text"
      },
      "source": [
        "ExtraTrees gave the worst score because I didn't tune the parameters, I will try to tune them in future versions and see if we can use it in stacking and averaging. For now, we discard it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoIYUlc3Zm6Y",
        "colab_type": "text"
      },
      "source": [
        "### VOTING REGRESSOR:\n",
        "* A voting regressor is an ensemble meta-estimator that fits base regressors each on the whole dataset. It, then, averages the individual predictions to form a final prediction.\n",
        "\n",
        "* After running the regressors, we combine them first with voting regressor in order to get a better model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "dgl-6Kb5Zm6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import VotingRegressor\n",
        "\n",
        "vote_mod = VotingRegressor([('Ridge', ridge_mod), ('Lasso', lasso_mod), ('Elastic', elastic_cv), ('XGBRegressor', xgb)])\n",
        "vote= vote_mod.fit(x_train, y_train.ravel())\n",
        "vote_pred=vote.predict(x_test)\n",
        "\n",
        "print('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, vote_pred))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "924CBZqHZm6a",
        "colab_type": "text"
      },
      "source": [
        "**NICE!! This is the best RMSE score so far. RMSE=0.1101**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYKCwIOsZm6a",
        "colab_type": "text"
      },
      "source": [
        "### STACKING REGRESSOR:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENYb98gzZm6b",
        "colab_type": "text"
      },
      "source": [
        "We stack all the previous models, including the votingregressor as the meta regressor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pwmsULfOZm6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mlxtend.regressor import StackingRegressor\n",
        "\n",
        "\n",
        "stregr = StackingRegressor(regressors=[elastic_cv,ridge_mod, lasso_mod], \n",
        "                           meta_regressor=vote_mod, use_features_in_secondary=True)\n",
        "\n",
        "stack_mod=stregr.fit(x_train, y_train.ravel())\n",
        "stacking_pred=stack_mod.predict(x_test)\n",
        "\n",
        "print('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, stacking_pred))))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8b-JMIQZm6d",
        "colab_type": "text"
      },
      "source": [
        "The RMSE score is good but not as good as VotingRegressor, we will average both of them for a stronger model.\n",
        "* Last thing to do is fit our regressors on the testing dataset and then average them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "KtLb2BpWZm6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#VotingRegressor to predict the final Test\n",
        "vote_test = vote_mod.predict(X_test)\n",
        "final1=np.expm1(vote_test)\n",
        "\n",
        "#StackingRegressor to predict the final Test\n",
        "stack_test = stregr.predict(X_test)\n",
        "final2=np.expm1(stack_test)\n",
        "\n",
        "#LassoRegressor to predict the final Test\n",
        "lasso_test = lasso_mod.predict(X_test)\n",
        "final3=np.expm1(lasso_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlEq9zg_Zm6f",
        "colab_type": "text"
      },
      "source": [
        "I will create 2 csv files and test which one gives better results:\n",
        "* 1: The VotingRegressor prediction (since it gave the best score)\n",
        "* 2: The average prediction of the Voting/Stacking/Lasso (Averaging models tends to lower the error rates)\n",
        "- We will submit both results and see which one gives the best LB score: **Voting** or **Averaging**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "LM8vVVhEZm6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Submission of the results predicted by VotingRegressor\n",
        "vote_submission = pd.DataFrame({\n",
        "        \"Id\": b[\"Id\"],\n",
        "        \"SalePrice\": final1\n",
        "    })\n",
        "vote_submission.to_csv(\"sample_submission.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsa3U48iZm6h",
        "colab_type": "text"
      },
      "source": [
        "Averaging the stacking, voting and lasso models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "FFNflcj0Zm6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Submission of the results predicted by the average of Voting/Stacking/Lasso\n",
        "final=(final1+final2+final3)/3\n",
        "\n",
        "final_submission = pd.DataFrame({\n",
        "        \"Id\": b[\"Id\"],\n",
        "        \"SalePrice\": final\n",
        "    })\n",
        "final_submission.to_csv(\"sample_submission2.csv\", index=False)\n",
        "final_submission.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pPtLswsZm6j",
        "colab_type": "text"
      },
      "source": [
        " **Finally**, after submitting both csv files, the VotingRegressor alone gave a better score 0.1164. \n",
        " * The Voting regressor fits base regularization regressors: Ridge, Lasso and ElasticNet, each on the whole dataset. It, then, averages the individual predictions to form a final prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQZet2OtZm6j",
        "colab_type": "text"
      },
      "source": [
        "## If you made it this far, thank you for your attention.\n",
        " **In order to improve my LB score, I will keep updating this kernel by:**\n",
        "* **Creating new features**\n",
        "* **Detecting other outliers**\n",
        "* **Trying new regressors and stacking/blending them**"
      ]
    }
  ]
}